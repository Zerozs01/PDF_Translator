# Deep Research on Reducing “Ghost Text” in Tesseract-Based OCR Pipelines

## What “ghost text” usually means and why it happens

In practice, “ghost text” (text ผี) in a Tesseract OCR pipeline usually means **spurious characters/words that appear in the OCR output even though the corresponding image region does not contain real text**, or contains only weak text-like patterns (borders, noise, halftone texture, compressed artifacts, bleed-through, shadows). This is rarely a single bug; it is typically an interaction between (1) *image artifacts*, (2) *layout/segmentation mistakes*, and (3) *a recognizer that is forced to emit some hypothesis unless you add rejection logic*. citeturn7view1turn7view3turn16view0turn15view0

Tesseract’s own documentation describes several “non-text becomes text” traps that map directly to ghosting. Examples include **dark scanning borders** being “picked up as extra characters,” skew hurting line segmentation (which cascades into garbage recognition), and overly large backgrounds around tiny text triggering “empty page” or other failures that can manifest as odd outputs downstream. citeturn16view0turn7view3turn7view4

A second meaning of “ghost text” (especially in overlay workflows like your Phase 1) is **layout-driven ghosting**: duplicated, mispositioned, or drifting overlays caused by OCR done on the wrong region geometry (e.g., full-page OCR in multi-column layouts; OCR bounding boxes that are too tight and clip characters; reading-order mismatches). This is why Tesseract guidance repeatedly pushes ROI-style use (“OCR a small region” → set an appropriate segmentation mode, add a small border, avoid unreasonable borders). citeturn16view2turn7view4turn11view3

Finally, Tesseract’s own training docs explicitly acknowledge a kind of **model “hallucination” behavior**: systematic insertion or alteration patterns (e.g., unwanted capitalization, added spaces) can arise when the training texts overrepresent specific forms; the recommended mitigation is diverse, cleaned training data. This is conceptually different from image-noise ghosting, but it matters if you fine-tune models for manga fonts or niche typography. citeturn15view0turn13view2

## Image preprocessing methods that measurably lower error and spurious character detection

Tesseract does internal preprocessing (via Leptonica), but its docs emphasize that there are cases where the internal steps are not enough. They recommend **external preprocessing** and provide a built-in debug knob: you can write out the internally processed image (`tessedit_write_images=true`) to inspect whether thresholding, borders, deskew, etc., are producing a “bad tessinput,” which strongly predicts garbage OCR. citeturn7view1turn7view2

A major evidence-backed lever is **binarization under uneven illumination**. A peer-reviewed study on non-uniformly illuminated documents evaluated many thresholding/binarization methods *using Tesseract as the OCR engine* and reported large quality differences depending on binarization choice. In their results table, global Otsu thresholding performed poorly on the tested dataset (OCR accuracy around ~0.52), while adaptive methods such as Sauvola and NICK produced substantially higher OCR accuracy (around ~0.90+ in their table). The key point for ghost reduction is straightforward: better binarization suppresses background texture/shadows that otherwise get interpreted as strokes. citeturn2view6turn12view1

Research also supports a **pipeline view** rather than a single “magic filter.” An MDPI study analyzing OCR on electronic IC labeling (again using Tesseract) performed an ablation study and found that multiple preprocessing stages mattered; in particular, removing binarization or scaling caused large performance drops, and “minor” steps like **image straightening (deskew), morphological border cleaning, and morphological noise removal** still contributed to the final improvement. This lines up tightly with ghost-text behavior you see in scanned pages: small speckles and border gradients become characters unless you remove them explicitly. citeturn12view2turn2view3

More advanced approaches treat preprocessing as a **learned selection or optimization problem**. A 2020 paper in entity["organization","MDPI","academic publisher"]’s journal *Symmetry* proposed an adaptive convolution-based preprocessing method guided by reinforcement learning, explicitly optimizing recognition quality by minimizing edit distance to ground truth. They report large gains on a challenging dataset, illustrating that “choose/compose preprocessing transforms adaptively” can be a principled approach instead of hand-tuning one fixed pipeline. citeturn7view0turn12view3

At the “document restoration” end (useful for very degraded scans, bleed-through, heavy noise), a 2025 arXiv pipeline (“PreP-OCR”) combines a restoration model with a semantic-aware post-OCR corrector (ByT5). On a large evaluation (13,831 pages), they report character error rate reductions on the order of ~64–70% compared to OCR on raw images. While this is not Tesseract-specific, it is directly relevant if your pipeline must handle low-quality manga scans or archival PDFs where classic preprocessing is insufficient. citeturn12view4turn2view7

image_group{"layout":"carousel","aspect_ratio":"16:9","query":["document binarization Otsu vs Sauvola example","scanned page dark border artifact OCR example","deskew scanned document before after","CRAFT text detection bounding boxes example"],"num_per_query":1}

## Detection-first OCR: why “crop and OCR regions” is the highest leverage ghost-killer

Across both Tesseract guidance and the broader OCR literature, the most consistent “ghost killer” is **not OCR’ing the whole page**. Instead: detect text/layout regions first, crop them, add sane padding, deskew locally if needed, then recognize. This reduces the search space for the recognizer and prevents background structures from being interpreted as characters. Tesseract’s docs explicitly recommend changing segmentation modes when OCR’ing small regions and warn about border conditions (too tight vs too large), which is essentially a lightweight form of “detection-first” thinking. citeturn16view2turn7view4turn7view3

Strong empirical evidence comes from a 2025 study on complex-layout historical documents (published as a PDF via the German National Library site) that tested multiple scenarios: full-page OCR vs OCR after layout detection/segmentation. Their results table shows that **adding layout detection as a preprocessing step** (segmenting the page into meaningful blocks and feeding snippets to Tesseract) substantially improved both CER and WER compared to full-page OCR—even when the Tesseract model was already fine-tuned. They also report a small but impactful detail that matters for ghosts-and-overlay: adding **a tiny padding (two pixels) around predicted boxes** helped because “very accurate” boxes may still clip character strokes at borders. citeturn11view2turn11view3turn9view4

A complementary perspective comes from the “Text Detection Forgot About Document OCR” paper, which argues (based on evaluation on structured document benchmarks) that modern “in-the-wild” text detection methods can be competitive on documents and can outperform some available document OCR methods, reinforcing the idea that **state-of-the-art detectors are valuable even in document-style pipelines**. For your Knowledge3 Phase 1, the practical implication is: a good detector is not “extra complexity,” it is the gate that prevents ghost explosions. citeturn2view4

Concrete detector families that support detection-first OCR—each producing boxes/polygons you can store as first-class geometry—include:
- EAST (predicts oriented rectangles/quadrangles efficiently) citeturn4search2turn4search6  
- CRAFT (character-region + affinity modeling; strong at finding text in complex layouts) citeturn4search4turn4search0  
- DBNet / differentiable binarization detectors (segmentation-based detection with integrated binarization behavior) citeturn4search1turn4search9  

These papers are not “about Tesseract ghosts” by name—but algorithmically they target the *root cause*: stop feeding background clutter into the recognizer. citeturn2view4turn11view2

## Tesseract-side levers: segmentation modes, models, and settings that suppress hallucinations

Even with detection-first, Tesseract configuration strongly affects ghost rate because page layout assumptions drive which connected components get grouped into “text.” Tesseract’s “ImproveQuality” guide emphasizes that Tesseract expects a page of text by default; when OCR’ing small regions you should select a suitable `--psm` mode, and it even lists modes such as “single uniform block,” “single line,” “sparse text,” and “raw line.” In ghost-heavy scenarios, picking the wrong segmentation mode is equivalent to telling the engine to invent structure where none exists. citeturn16view2turn2view0

Tesseract’s own docs also call out several image-format conditions that can induce bad recognition:
- For modern versions, prefer **dark text on a light background** rather than inverted polarity. citeturn7view2  
- Work near **300 DPI (or rescale appropriately)**. citeturn7view2turn11view3  
- Handle **alpha channels** carefully; even though newer Tesseract versions remove alpha internally, blending behavior can still cause problems in some cases. citeturn7view4turn16view0  
- Remove / crop **dark scanning borders**—explicitly noted as a source of extra-character pickup. citeturn16view0turn7view3  

Model choice matters too. The official traineddata repository documentation states that the integerized LSTM models are updated from the “best” set and are “probably a little less accurate,” while the `tessdata_fast` set trades accuracy for speed with a smaller network. For ghost reduction (a quality-first objective in your Phase 1), this implies preferring “best” when latency allows, and using “fast” only when you can compensate with stronger gating/post-filters. citeturn2view1turn1search5

If you fine-tune models (common for stylized manga fonts or historical typefaces), Tesseract’s documentation explicitly warns about a “hallucination effect” in 4.x training behavior: skewed training distributions (e.g., overrepresented capitalization or leading/trailing spaces) can cause systematic misbehavior, and the mitigation recommended is diverse, cleaned training text. In other words: fine-tuning can reduce errors, but it can also *create new ghosts* if training data is biased or dirty. citeturn15view0turn13view2

Finally, Tesseract provides dictionary/pattern controls that can reduce spurious outputs when your domain doesn’t match “natural language sentences.” The ImproveQuality guide notes you can disable system/frequent word dictionaries (`load_system_dawg`, `load_freq_dawg`), add expected words/patterns, and restrict recognized characters via `tessedit_char_whitelist`. Character-set restriction is one of the simplest ways to prevent “random punctuation salad” ghosts in known-format regions (e.g., page numbers, prices, IDs). citeturn16view2

## Post-OCR filtering: confidence gating, morphological checks, and sequence correction

Because OCR engines can emit text even from borderline visual evidence, production pipelines usually treat OCR output as **probabilistic** and add rejection/cleanup stages. A 2023 paper in entity["organization","Association for Computational Linguistics","academic society"] proceedings emphasizes that OCR confidence is “not an absolute measure”: low confidence can still be correct, and high confidence can still be wrong; nonetheless, confidence is commonly used as a decision signal because it reflects the engine’s internal assessment of correctness. This is the core justification for your “confidence gating is mandatory” principle in Knowledge3. citeturn12view5turn3view1

There is also good evidence that **language-aware or lexicon-aware filtering** reduces downstream harm from OCR noise (including ghosts). Work on Finnish historical newspaper re-OCR (distributed as a CEUR-WS PDF; developed in the context of the entity["organization","National Library of Finland","Helsinki, Finland"]) describes a re-OCR process whose components include multiple image preprocessing techniques, OCR, candidate selection, and crucially **morphological analyzers plus character-level weighting rules**. They report improved OCR quality metrics (including CER/WER) and show that morphology-based recognition rates can improve substantially compared to baseline OCR. For your pipeline, the transferable idea is: “is this output word plausible in my language/domain?” is a powerful ghost suppressor—especially before translation. citeturn11view0turn11view1turn11view1turn11view1turn11view1turn11view1

Error-correction research in NLP also supports post-OCR correction as a structured problem. A 2017 paper on historical Finnish OCR combines OCR output with data-driven spelling correction using weighted finite-state methods, achieving high character recognition accuracy in their best configuration. While their OCR engine was not Tesseract in that experiment, the architecture generalizes: (1) OCR generates noisy text (including insertions), (2) a correction model penalizes implausible strings and reduces insertions/substitutions. This is directly relevant to “ghost tokens”: inserted characters/words are exactly what edit-distance–style correction tries to delete. citeturn9view2turn11view1

On evaluation methodology, the CEUR-WS re-OCR paper explicitly defines and uses **CER/WER** and frames them in terms of insertions/substitutions/deletions needed to transform OCR output into reference text (edit-distance family). This is useful for your roadmap because “ghost text” is often dominated by **insertions**, so tracking insertion-heavy error profiles (not just overall CER) gives a clearer signal that your ghost-suppression work is working. citeturn11view1turn11view3

## How to fold these research-backed techniques into your Knowledge3.md roadmap

Your Knowledge3.md already emphasizes detection-first OCR, geometry-first overlay, and confidence gating. The research above mostly validates that architecture and suggests a few specific upgrades that are unusually high-impact for ghost reduction.

For Phase 1 (“OCR overlay + minimal ghosts”), the most evidence-backed additions are:
- **Make cropping/padding a first-class algorithm**, not a convenience. The 2025 complex-layout paper shows that running OCR on segmented snippets improves CER/WER, and that even a tiny padding around boxes can avoid clipped strokes that lead to recognition noise (and misalignment in overlays). citeturn11view2turn11view3  
- **Treat border removal as ghost prevention**, not cleanup. Tesseract’s docs explicitly state scanned dark borders can be “picked up as extra characters,” which is ghost text in the most literal sense. Putting border detection/removal ahead of OCR is therefore a justified “non-negotiable” step for scanned pages. citeturn16view0turn7view3  
- **Instrument Tesseract preprocessing output** (`tessedit_write_images`) so you can debug when internal thresholding/deskew fails; this reduces time wasted on model-side guessing when the real cause is image conditioning. citeturn7view1turn7view2  

For Phase 2 (“Translate + Replace Overlay”), the main research-driven reinforcement is: you should **not translate low-confidence or morphologically implausible tokens by default**, because confidence is designed as a probability-like decision aid (even if imperfect), and language-aware checking is a proven strategy in large-scale re-OCR workflows. citeturn12view5turn11view1turn11view0

For Phase 3 (“Manga Editor Mode”), detection-first becomes even more important because manga pages contain dense non-text structure (panels, screentones, art textures) that can trigger spurious OCR. Modern detection models like EAST/CRAFT/DBNet are explicitly designed to localize text under complex backgrounds and arbitrary orientations; using them (or a document-layout detector) as your “text/no-text gate” is the algorithmic way to stop full-page hallucinations. citeturn4search2turn4search4turn4search1turn2view4

For Phase 4 (“PSD export”), two research-aligned notes matter for overlay correctness: (1) keep geometry stable and explicit, and (2) store coordinates in a clear, interoperable coordinate system. The hOCR spec precisely defines `bbox` coordinates (x0 y0 x1 y1) relative to the top-left of the image in pixels, and is explicitly intended to represent OCR results alongside layout analysis. Even if you don’t output hOCR, matching its coordinate conventions reduces ambiguity when mapping OCR geometry into canvas coordinates and later into PSD layer geometry. citeturn17view0turn17view1